# 실시간 데이터 수집 및 처리 파이프라인 구현
중고나라 웹 사이트에서 상품 정보를 수집하여 카프카로 전달하고 카프카 커넥트의 mysql-sink-connector를 통해 db에 저장하며 카프카 스트림즈를 통해 토픽에 들어오는 상품 데이터 중 100만원 넘는 것을 제외하고 누적 합계를 계산하여 카프카로 다시 저장하는 파이썬과 도커 기반의 데모 프로젝트

# docker compose 환경
- kafka-ui: 카프카 웹 UI 매니저 (카프카 노드 상태 확인 가능)
- akhq: 카프카 웹 UI 매니저 (토픽 삭제, 컨슈머랙, 토픽 데이터 확인, 노드 런타임 설졍 변경 가능)
- schema-registry: 카프카 데이터 메시지의 스키마 정의를 저장, 관리, 싱크하는 서비스
- kafka-[1,2,3]: 3개의 카프카 브로커로 구성 (토픽 기반 데이터 저장, 전달, 통지하고 할당 파티션 담당. 생산/소비자의 싱크/소스 역할)
- zookeeper-[1,2,3]: 3개의 주키퍼 노드 (카프카 브로커들을 제어하고 관리하는 역할)
- ELK: 서비스 설정은 하였으나 시간 상 활용 못함
- mysql: 카프카 토픽이 유인된 데이터를 카프카 커넥터를 통해 db로 저장 보관
- connect: 카프카 커넥터들을 등록하고 관리할 수 있는 서비스

# 스택 선정 이유 (카프카)
- 버퍼 큐잉 역할: 
  - 대량의 데이터를 상당 기간 보관하는 저장소 역할을 하면서도 제로 카피를 통해 고성능 처리량을 구현
  - 분산 클러스터 환경으로 일시적으로 높은 처리량이 요구되는 경우 적응형 대응 가능
  - 급작스럽게 생산량이 증가하는 경우 DB에 직접 쓰는 경우는 처리량을 초과하면 장애가 발생하지만
  - 카프카의 경우 데이터를 보존하고 있으므로 데이터 지연(컨슈머랙)은 있겠지만 DB 본래의 속도에 따라 처리 가능
- 서비스간 느슨한 연결:
  - 기존에 각 데이터 형태에 맞는 분리된 각각의 파이프라인을 가져갔다면 (서비스A - 디비A, 서비스B - 디비B, 디비A+디비B - 서비스C)
  - 중앙 통제형의 카프카 이벤트 허브를 활용하면 모든 서비스가 카프카만 인지/연결되면 되어 아키텍처 복잡도를 단순하게 유지
  - 서로 다른 부서에서 다각도의 데이터를 종합하여 활용 가능 (할 것으로 예상) 
- 장애에 대한 대응:
  - ack=all과 같은 옵션을 통해 네트워크 장애 등의 의한 누락없이 카프카에 전달된 것을 보장하는 방법 제공 
  - 레플리카 설정에 따른 브로커 다운/디스크 장애 상당한 수준의 대응 가능 (레플리카 factor와 isr 값을 통해 조절 가능)
    - 단, min.insyc.replica의 수는 레플리카 팩터보다 최소 1 적어야 ACK=all 설정일 때 전체 장애 회피 가능
  - 데이터 retention 기간이 있으므로 카프카 이후 단계의 인프라가 다운되도 다시 이어서 재시작 가능
- 계층적 성능, 유지관리 용이 특성:
  - 제로카피를 활용하므로 파티션 내 최근 데이터는 메모리에 존재하므로 실시간 처리 성능이 좋은편
  - 이는 잦은 실시간성 읽기 요청은 메모리에서 수행되고 비교적 bounded 데이터만 디스크에 쓰기/읽기되므로 디스크 수명에 도움
  - 이런 특성으로 핫데이터의 실시간 처리와 확정된 데이터의 헤비한 배치 처리 각각에 유연함  
  - 디스크로 쓰여져 확정된 데이터에 대해 필요시 S3 디스크나 테이프 백업으로 장기 저장
  - 키의 분포가 균등하면 요청이 특정 파티션으로 몰리지 않고 고르게 분산되어 부하 분산 효과

    
# producer (scraper/main.py)
- 실행: (project_root)$ python -m scraper.main
- 요약: trio 비동기 방식으로 중고나라 상품 정보를 파싱하여 수집하고 카프카로 전달
- 환경: python 3.13.2
- 설정: hydra 라이브러리를 통한 scraper/config.yaml 설정 로드 (아직 코드에 하드코딩되었거나 일원화 되지 않은 값들 존재)
- 미구현 항목 (중복 방지)
  - 재시작, 병렬 실행의 상황인 경우 중복된 데이터가 카프카로 전송될 수 있음
  - 작업 오프셋(단조 증가 속성의 상품 아이디)을 영속화/로드하여 일정 수준의 중복 방지 필요
  - 좀 더 강력한 수준의 생산자측 중복 방지를 위해서는 상품 정보가 아닌 아이디 갱신 값만 카프카로 전달하고
  - 카프카나 인메모리 DB 기록에서 [마지막-작업된-아이디+1, 신규-아이디]에 대해 수집을 트리거하는 방식으로 가능
  - 다만, 비교적 가벼운 데이터라면 중복을 포함한 분산 수집을 허용하고 카프카 이후 단계에서 타임 윈도우를 가지고 필터링 하는 것이 유효할 듯
- 미구현 항목 (장애 실패, 데이터 누락 케이스)
  - 기본적으로 데이터 패치 실패 시 n회의 고정기간 재시도 / 비선형 시간 증가를 반영한 m회의 재시도 (백오프)
  - 실패 머신이 아닌 외부 머신/아이피에서 데이터 체크 
  - ip 블럭을 회피하기 위한 IP rotator
  - 분산 수행되는 경우 문제의 워커는 정지하고 다른 리전, 프로바이더에 워커 활성화
- 미구현 항목 (오류, 경계 상황 알림)
  - 시작, 실패, 종료 등을 정의하여 카프카로 이벤트 전달 -> 시각화 도구와 슬랙 등의 매신저 통지
 
# consumer (scraper/scripts/mysql_sink...)
- 실행: (project_root/scraper/scripts)$ ./regsiter_mysql_sink_connector.sh
- 요약: 카프카 커넥트 노드에 등록되어 상시 카프카 토픽에 데이터가 들어오면 mysql로 저장하는 커텍터
- 이슈 기록:
  - avroProducer를 통하면 스키마 레지스트리에 스키마 자동 등록
  - 이 스키마를 통해 커넥터가 데이터 디코딩
  - 직접 avro 인코딩한 데이터를 producer로 전송한 경우 커넥터에서는 디코딩 실패 이슈
- 미구현 항목 (스키마 레지스트리 의존성 추가 필요)
- 미구현 항목 (DB 스키마 설계, 정규화, 4단계 카테고리 분할 등)

# consumer (scraper/streams/filtered_sum.py)
- 실행: (project_root)$ python -m scraper.streams.filtered_sum
- 환경: python 3.11.11 (bytewax 파이썬 3.13 지원 안하는 이슈)
- 요약: 앱 실행 중 카프카 토픽에 데이터가 들어오는 순간 배치 단위로 누적 금액 계산
- 이슈 기록:
  - faust 라이브러리로 작업하였으나 deprecated된 것으로 실제 동작하지 않았음
  - bytewax의 경우 파이썬 3.13에서 오래된 패키지만 제공되고
  - rust 파일에 cpython 변경 사항을 수정해서 따로 빌드해야 pip 설치가 성공함
  - 하지만 그럼에도 현재 API와 너무 달라 파이썬 3.11 가상 환경을 통해 수행
- 미구현 항목 (토픽이 여러 파티션일 경우 각 컨슈머에 따른 합계 아이디 부여)

# 기타 미구현 / 고려 필요 사항
- docker-compose.yml 설정
  - 처음에 내부 네트워크 고정 아이피로 할당하지 않아서 재시작 시 주소가 변경되는 케이스 발생  
  - 스키마 레지스트리(+비슷한 유형의 서비스)경우 사용처에서 캐싱을 하지만 SPoF가 될 수 있으므로 클러스터 구성 필요
- 데이터 중복 제어
  - 1. 프로듀서 enable.idempotence 활성화
  - 2. 프로듀서 고유한 transactional.id 설정 (트랜잭션 활성화, 메시지 단위가 아닌 프로듀서 인스턴스 부여)
  - 3. 컨슈머 파라미터에 isolation.level = read_committed 설정
  - 4. 컨슈머 파라미터에 enable.auto.commit = false 설정
  - 필요하다면 in-memory db를 통해 장부 활용을 통한 고유키 체크, 작업 범위 분할, 락 등의 구현 가능할 듯?
- 데이터 시간차 제어
  - 여러 센서의 데이터가 시간 오차나 누락으로 데이터 구간이 1:1 매핑이 되지 않는 경우
  - DTW (dynamic time warping) 알고리즘으로 n, m 구간 매핑 최적화
- 보호 데이터 암호화
  - 솔트값을 사용하여 SHA256 encryptor, decryptor 활용
- 부하 처리
  - 쿠버네티스 환경에서 컨슈머가 운용된다면 카프카의 리소스 사용율과 컨슈머랙 등의 지표르리 기준으로 파티션 수까지 컨슈머를 오토 스케일링
  - 생산자의 경우 평소 카프카 클러스터의 최대 처리량을 미리 계산해두고 몇 단계의 임계점을 두어 이를 초과하는 모니터링 알람이 있을 경우
  - 브로커를 추가하고 데이터 채널이 분리될 수 있는 형태라면 동적으로 토픽을 운영하는 전략 수행
 

 
  
 
